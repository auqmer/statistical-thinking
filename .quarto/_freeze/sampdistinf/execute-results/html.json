{
  "hash": "76abbabe59dba06f7da9664e445f48ee",
  "result": {
    "markdown": "---\ntitle: \"Sampling Distributions and Statistical Inference\"\n---\n\n::: {.cell}\n\n:::\n\n\nR packages used in this chapter:\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(knitr)\nlibrary(mosaic)\nlibrary(Lock5Data)\nlibrary(supernova)\nlibrary(DiagrammeR)\nlibrary(psych)\n```\n:::\n\n\n## Sampling Distributions of Error\n\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/ermahtSP20.Rdata\") # This file can be found on Canvas\nerma <- ermahtSP20\nerma$e <- erma$height - mean(erma$height)\nerma$Eps <- erma$height - 67\n```\n:::\n\n\n## Adult Heights from NHANES Study\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(psych)\nlibrary(NHANES)\ndata(\"NHANES\")\nheight_in <- NHANES[NHANES$Age >= 21, ]$Height*.393701\ndescribe(height_in, fast = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   vars    n  mean   sd   min  max range   se\nX1    1 7041 66.45 3.98 52.95 78.9 25.94 0.05\n```\n:::\n:::\n\n\n$\\mu =$ 67\n\n$\\sigma =$ 4\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nprint(erma[ ,c(1,3)])\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   id height\n1  ek   65.0\n2  lz   69.0\n3  lk   67.5\n4  bw   75.0\n5  kb   62.5\n6  dg   68.0\n7  cs   72.0\n8  ao   67.0\n9  wb   70.0\n10 tm   72.0\n11 nb   66.5\n12 pm   68.5\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ndescribe(erma$height, fast = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n   vars  n  mean   sd  min max range   se\nX1    1 12 68.58 3.37 62.5  75  12.5 0.97\n```\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nload(\"data/ermahtSP20.Rdata\")\n\nmean(ermahtSP20$height)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[1] 68.58333\n```\n:::\n\n```{.r .cell-code}\nermahtSP20$height\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] 65.0 69.0 67.5 75.0 62.5 68.0 72.0 67.0 70.0 72.0 66.5 68.5\n```\n:::\n\n```{.r .cell-code}\nround(ermahtSP20$height - mean(ermahtSP20$height),1)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n [1] -3.6  0.4 -1.1  6.4 -6.1 -0.6  3.4 -1.6  1.4  3.4 -2.1 -0.1\n```\n:::\n:::\n\n\n\n## Demonstration of sampling distrubtions\n\nTo play around with sampling distributions, go to the following website, scroll down the page to the section called **Sampling Distributions and Central Limits Theorem** and click on the image called \"Sampling Distribution of the Sample Mean (Continuous Population)\"\n\n[https://artofstat.com/web-apps](https://artofstat.com/web-apps)\n\n## Standard Error of the Mean (SEM or S.E.)\n\n#### Population formula:\n\n$$\\sigma_{\\bar{x}} = \\frac{\\sigma}{\\sqrt{n}}$$\n\n#### Sample formula:\n\n$$s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}$$\n\n## Properties of Estimators\n\n* **Unbiased**: The peak of the sampling distribution equals the parameter value\n* **Efficient**: Most of the estimates are close to the parameter\n* **Consistency**: The more observations the closer the estimates are to the parameter, or said differently, \nthe bigger the sample size the narrower the sampling distribution\n\n## Assumptions of Linear Models\n\n* Normality\n* Independence of Errors\n* Identical Distribution of Errors/Homogeneity of Variance\n* Unbiased Errors\n\n## Normality\n\n* We assume the ERRORS are normally distributed\n\n* This is reasonable because:\n  - abundance of empirical evidence of normal errors\n  - possible to transform variables to have normal errors\n  - **Central Limits Theorem**\n  \n* **Central Limits Theorem** - sampling distributions of the sample mean approximate the normal distribution regardless of the shape of the population distribution. \n* The larger the number of components that go into an average the better this approximation (think unmeasured factors and sample size).\n\n## Independence of Errors\n\n* Knowing one observation’s error tells us nothing about another observation’s error\n* Violation Examples\n    + Positive: Couple’s political attitudes\n    + Negative: Couple’s housework estimates\n* Care whenever observations are linked in any way\n\n## Identical Distribution of Errors\n\n* Equal variances for all groups\n    + size of error not related to size of model prediction\n    + Violations: counts, reaction times, money\n* Homogeneity of Variance\n\n## Unbiased Error\n\n* Mean of Errors = 0\n* Otherwise just becomes part of MODEL\n* A researcher issue rather than a data analysis or statistical issue\n* So YOU take care to ensure no bias\n\n## Estimator for $\\sigma^2$\n\n$$\n\\hat{\\sigma}^2 = s^2 = \\frac{SSE}{n-1} = \\sum\\frac{(Y_i - \\bar{Y})^2}{n - 1}\n$$\n$$\n\\hat{\\sigma}^2 = MSE = \\frac{SSE}{n-p} = \\sum\\frac{(Y_i - \\hat{Y_i})^2}{n - p}\n$$\n\n## Why $n-p$ instead of $n$?\n\n$$\nDATA = MODEL + ERROR\n$$\n\n$$\nn = p +(n-p)\n$$\n\n* Only $(n-p)$ independent pieces of information left in ERROR\n\n\n# Hypothesis Testing vs. Modeling\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninclude_graphics(\"images/repCrisis.png\")\n```\n\n::: {.cell-output-display}\n![](images/repCrisis.png){width=68%}\n:::\n:::\n\n\n## A Problems in Science\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninclude_graphics(\"images/manylabs3.png\")\n```\n\n::: {.cell-output-display}\n![](images/manylabs3.png){width=80%}\n:::\n:::\n\n\n\n## Overview\n\n1. Hypothesis Testing and  p-values\n2. Modeling\n3. Linear Regression\n\n## Hypotheses\n\n* An hypothesis is a testable statement about a **population**.\n* It takes the form of a prediction about the value or range of values a **parameter** takes.\n\n## The *p*-value\n\nA *p*-value is the probability of obtaining data as extreme or more extreme as obtained in the sample given that the null hypothesis is true. \n\n*p*-value :\n\n$$\n p = P(D | H_0)\n$$\n\n## Observational Learning Albert Bandura (1965)\n\n### Influence of Models' Reinforcement Contingencies on the acquisition of Imitative Responses\n\nIn order to test the hypothesis that reinforcements administered to a model\ninfluence the performance but not the acquisition of matching responses, groups\nof children observed an aggressive film-mediated model either rewarded,\npunished, or left without consequences. A postexposure test revealed that\nresponse consequences to the model had produced differential amounts of\nimitative behavior. Children in the model-punished condition performed significantly fewer matching responses than children in both the model-rewarded\nand the no-consequences groups. Children in all 3 treatment conditions were\nthen offered attractive reinforcers contingent on their reproducing the model's\naggressive responses. The introduction of positive incentives completely wiped\nout the previously observed performance differences, revealing an equivalent\namount of learning among children in the model-rewarded, model-punished,\nand the no-consequences conditions.\n\n## Hypotheses\n\n$H_0$: ???\n\n* $H_1$: It was predicted that reinforcing consequences to the model would result in significant differences in the performance of imitative behavior with the model-rewarded group displaying the highest number of different classes of matching responses, followed by the no-consequences and the model-punished groups, respectively.\n\n* $H_2$: In accordance with previous findings (Bandura et al., 1961, 1963a) it was also expected that boys would perform significantly more imitative aggression than girls.\n\n* $H_3$: It was predicted, however, that the introduction of positive incentives would wipe out both reinforcement-produced and sex-linked performance differences, revealing an equivalent amount of learning among children in the three treatment conditions.\n\n\n### Figure 1. Mean number of different matching responses reproduced by children as a function of positive incentives and the model's reinforcement contingencies. \n\n\n::: {.cell}\n\n```{.r .cell-code}\ninclude_graphics(\"images/banduraBarplot.png\")\n```\n\n::: {.cell-output-display}\n![](images/banduraBarplot.png){width=50%}\n:::\n:::\n\n\n## Table 1 \n\n### ANOVA Results for Performance Differences\n\nSource | *df* | MS  | F\n-------|------|-----|----\nTreatment (T) | 2 | 1.21 | 3.27*\nSex (S) | 1 | 4.87 | 13.16**\nT X S | 2 | .12 | <1\nWithin groups | 60 | .37 | |\n\nNote: * *p* < .05; ** *p* < .001.\n\n## Table 2 \n### Comparison of Pairs of Means Between Treatment Conditions (*t* values)\n\nPerformance | Reward vs. punishment | Reward vs none | Punishment vs none\n-----|-----|-----|-----\nAll | 2.20** | 0.55 | 2.25**\nBoys | 1.05 | 0.19 | 1.24\nGirls | 2.13** | 0.12 | 2.02*\n\n## What does the $p$-value Tell Us?\n\n* the probability of obtaining a $F$-value as extreme, or more extreme as 3.27, **assuming the null hypothesis is true**\n\n* Said differently, in a world in which different reinforcement contingencies have exacly no effect on the number of aggressive imitations, a $F$-score of 3.27 or greater would occur less than once in one thousand times in a very large number of randoms samples of size 66.\n\n## What do $p$-values NOT tell Us?\n\nRecall:\n\n$$\np(D | H_0) \\ne p(H_0 | D)\n$$\n\n## Lottery\n\nW = winning the lottery\n\nT = Have a valid lottery ticket\n\n$$\np(W|T) \\ne p(T|W)\n$$\n\n## Lottery probabilities in Perspective\n\n> * The probability of winning the MegaMillions Georgia Lottery was $3.03 \\times 10^{-9}$ on 1/27/2019.\n> * The probability of being struck by lightening in your lifetime is $\\frac{1}{3000}$ ( $3.33 \\times 10^{-4}$ ).\n> * The probability of being struck by lightening **THIS YEAR** is $1.43 \\times 10^{-6}$.\n\n\n## Cognitive Errors in Significance Testing\n\n1. False belief that $p$ is the probability that a result happened by chance.\n\n2. False belief that rejecting the null means the likelihood that this decision was wrong is less than 5%.\n\n3. False belief that a $p$ value give the probability that the null hypothesis is true.\n\n4. False belief that $1-p$ is the probability that the alternative hypothesis is true.\n\n5. False belief that $1-p$ is the probability of finding the same result in another sample.\n\n## Model Comparison\n\n* Model A:\n\n$$\nY_i = \\beta_0 + \\varepsilon_i\n$$\n\n* Model C:\n\n$$\nY_i = B_0 + \\varepsilon_i\n$$\n\n$$\nH_0: \\beta_0 = B_0\n$$\n\n\n## Power\n\nhttp://www.artofstat.com/webapps.html\n\n## Punchline\n\n* All statistical inference for model comparisons is the same!\n* Compute PRE, F* from SSE(A) and SSE(C)\n* If PRE & F* surprising, then reject Model C\n* Always consider Power\n* Now let’s build some interesting MODELS (Golems)!\n\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}