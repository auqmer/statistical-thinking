[
  {
    "path": "posts/2020-09-16-central-tendency/",
    "title": "Measures of Central Tendency",
    "description": "Central Tendency is an important statistics in much of research. The goal is to simplify our data to one representative value. Before you do this you must make sure your measure of central tendency is   appropriate for your data. In this tutorial I will introduce you to the most common ways to calculate central tendency",
    "author": [
      {
        "name": "William Murrah",
        "url": "www.statistical-thinking.com"
      }
    ],
    "date": "2020-09-16",
    "categories": [],
    "contents": "\nPackages used in this tutorial\n\n\nlibrary(sn)\nlibrary(mosaic)\n\n\n\n\n\nsource(\"code/ct_hist.R\")\n\n\n\nNotation\nScientific notation, while often confusing and frustrating initially, is very useful in helping to convey complex ideas in a compact and precise manner. The table below contains scientific notations relevant to this tutorial on central tendency and the next on variability.\nWe will be using the following notation in this class:\nImportant Notation\nSymbol\nmeaning\n\\(y\\)\nDependent Variable\n\\(x\\)\nIndependent Variable\n\\(N\\)\nPopulation size\n\\(n\\)\nSample size\n\\(\\Sigma\\)\nSum of\n\\(\\mu\\)\nPopulation mean\n\\(M\\) or \\(\\bar{X}\\)\nSample mean\n\\(M_w\\) or \\(\\bar{X}_w\\)\nWeighted mean\nMean\nCalculating the mean\n\\[\n\\text{Population mean: }  \\mu = \\frac{\\Sigma x}{N}\n\\]\n\\[\n\\text{Sample mean: } \\bar{x} = \\frac{\\Sigma x}{n}\n\\]\n\\(x\\) = (1, 1, 2, 2, 2, 2, 3, 3)\n\\(n\\) = 8\nCalculate sample mean:\n\\[\n\\bar{x} = \\frac{\\Sigma x}{n} = \\frac{1+1+2+2+2+2+3+3}{8} = \\frac{16}{8} =  2 \n\\]\n\n\n# Create a vector of numbers named `scores`\nscores <- c(1, 1, 2, 2, 2, 2, 3, 3)\n# Calculate the mean 'by hand'\nsum(scores)/length(scores)\n\n\n[1] 2\n\n# Calculate the mean using `mean()`\nmean(scores)\n\n\n[1] 2\n\nMeans summarize a distribution\nWe can table scores to see the frequency distribution,\n\n\ntable(scores)\n\n\nscores\n1 2 3 \n2 4 2 \n\nwhich shows we have two scores that have the value 2, four scores that have the value 2, and two scores that have the value 3.\nWe can plot the frequency distribution as a bar plot.\n\n\nbarplot(table(scores))\ntext(0.5, 3.5, expression(bar(X) == 2))\n\n\n\n\nBoth the table and the plot suggest that a score of 2 represents a typical or central score in this distribution of scores.\nCharacteristics of the Mean\nChanging an existing score changes the mean\nAdding a new score or removing an existing score will change the mean, unless the value of this score is equal to the mean.\nAdding, subtracting, multiplying, or dividing each score in a distribution by a constant will cause the mean to change by that constant.\nThe sum of the differences of scores from their mean is zero.\nThe sum of the squared differences of scores from their mean is minimal.\n\nThis last point is particularly important, so I will demonstrate it with our scores variable.\n\n\nc_scores <- scores - mean(scores) # Scores centered around the mean.\ncbind(scores, c_scores) \n\n\n     scores c_scores\n[1,]      1       -1\n[2,]      1       -1\n[3,]      2        0\n[4,]      2        0\n[5,]      2        0\n[6,]      2        0\n[7,]      3        1\n[8,]      3        1\n\nsum(c_scores)\n\n\n[1] 0\n\nBecause the mean is like the center of mass of the data, the scores above the mean cancel out the scores below the mean.\nMedian\nThe median is the middle score, with half of the scores falling above and half below this score. To calculate the median first order the values of the variable from smallest to largest. If n is odd, the median is the middle value in the sorted variable, or the value that is in the \\(\\frac{n + 1}{2}\\) position.\nIf the n is even, then the median value is the average of the middle two values of the sorted variable. So, for our example variable x\n\\(x\\) = (1, 1, 2, 2, 2, 2, 3, 3)\nbecause \\(n = 8\\), we average the middle two values, which are both \\(2\\), so the median of \\(x\\) is \\(2\\).\n\n\nmedian(scores)\n\n\n[1] 2\n\nMode\nThe mode is the most frequent value observed in a variable. NOTE: The mode() function in R does NOT calculate this form of central tendency, but does something very different. So don’t use that function to get the most common value in a variable. Instead you can use the table() function.\n\n\ntable(scores)\n\n\nscores\n1 2 3 \n2 4 2 \n\nChoosing a measure of central tendency\nWhen appropriate, the mean is the preferred measure of central tendency for most applications. This is because we can do the most with the mean mathematically. Recall from what we learned about the characteristics of the mean that we can meaningfully add, subtract, multiply and divide means. But this depends on the type of variable we are dealing with. Generally speaking, the mean is useful for numeric variables that are symmetric in shape, preferably normally distributed. When data are skewed (i.e. not symmetrical), the mean can be adversely impacted by the skewness, being pulled toward the skewed tail more so than the median. Therefore with skewed distributions the median can be a better measure of central tendency. The median is also less susceptible to the influence of outliers.\nAn illustrative example\nIf three people who earn typical salaries are sitting at a bar and Bill Gates sits down beside them, the average salary at the bar just went up drastically, and doesn’t represent anyone there. But the median salary, would still be a good representation of most of the people at the bar.\nThe median U.S. salary is about $45,000. Bill Gates makes about $3,710,000,000, or 3.7 billion dollars a year. We can “simulate” this in R:\n\n\nnormal_salaries <- c(40000, 45000, 50000)\nbillGates_salary <- 3.71e9 \n# Salaries after Bill Gates sits down at the bar\nawkward_conversation <- c(normal_salaries, billGates_salary) \n\n\n\nLet’s look at the measures of central tendency before Bill Gates strolls into the bar.\n\n\nmean(normal_salaries)\n\n\n[1] 45000\n\nmedian(normal_salaries)\n\n\n[1] 45000\n\nWith no outliers, both our measures give us a good sense of the typical salary at the bar.\nWhat happens when a billionaire sits down?\n\n\n# After Bill Gates sits down\nmean(awkward_conversation)\n\n\n[1] 927533750\n\nmedian(awkward_conversation)\n\n\n[1] 47500\n\nNow the mean salary is $927,533,750, which is way more than the three original patrons make, and much less than Bill Gates makes. So, the mean is not a good measure of central tendency here, but the median, which is $47,500 is still representative of the typical person at the bar.\nSkew and Central Tendency\nNotice from the example above, that outliers tend to pull the mean toward the tail of the distribution with the extreme cases. We can see that in the plots below.\n\n\nset.seed(1234)\nn <- 10000\nxbar <- 100\nsigma <- 15\nnrm <- round(rnorm(n, xbar, sigma),2)\n\npsk <- round(rsn(n, xi = xbar, omega = sigma, alpha = 5))\nnsk <- round(rsn(n, xi = xbar, omega = sigma, alpha = -5))\n\npar(mfrow = c(1, 3))\nct_hist(nrm, main = \"Normal\", ylim = c(0, 3000))\nct_hist(psk, main = \"Positive Skew\", ylim = c(0, 3000))\nct_hist(nsk, pos = \"topleft\", main = \"Negative Skew\", ylim = c(0, 3000))\n\n\n\npar(mfrow = c(1,1))\n\n\n\nWhen the distribution is symmetrical with no extreme values, the mean and median are typically good measures of central tendency, but with skew, the mean is pulled to the extreme tail more so than the median, and it is often beneficial to use the median. This is why you often hear about median income instead of mean income. You can use this to your advantage. Often we can get the mean and median of a distribution, and if they are substantially different, we can not only tell that there is skew, but we can tell what direction the skew is in. See if you can figure out how to do this from the graph above.\n\n\n\n",
    "preview": "posts/2020-09-16-central-tendency/distill-preview.png",
    "last_modified": "2021-01-11T13:26:08-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-23-variability/",
    "title": "Measures of Variability",
    "description": "While central tendency is important, we often want to understand more than just were the center of a distribution is located, we often also find useful measures of the spread of the values around the center. Such statistics are referred to as measures of dispersion or variability.  Common measures I will discuss in this tutorial include the range, interquartile range, variance, and standard deviation.",
    "author": [
      {
        "name": "William Murrah",
        "url": "www.statistical-thinking.com"
      }
    ],
    "date": "2020-09-16",
    "categories": [],
    "contents": "\nPackages Used\nI use the following packages in this tutorial.\n\n\nlibrary(psych)\nlibrary(ggplot2)\n\n\n\nImportant Notation\nScientific notation, while often confusing and frustrating initially, is very useful in helping to convey complex ideas in a compact and precise manner. The table below contains scientific notations relevant to measures of variability.\nSymbol\nmeaning\n\\(IQR\\)\nInterquartile range\n\\(\\sigma^2\\)\nPopulation variance\n\\(s^2\\)\nsample variance\n\\(\\sigma\\)\nPopulation standard deviation\n\\(s\\)\nsample standard deviation\nMeasures of Variability or Dispursion\nA measure of central tendency is not sufficient to describe a numerical variable.\nIn addition to information about the typical value, we also want to know how scores vary (or are dispersed) around the typical score.\nRange\nWe need some way to quantify the variability of the scores in distributions that reflects all scores and is sensitive to outliers. We already have a single score for the average or “typical” score - the mean. Ideally, we want a single number that represents the typical variation from the mean. The range is the simplest way to describe variability or how scores are dispersed across possible values. The range is the difference between the highest and lowest values in the variable.\n Range = Highest - Lowest\nThe range is useful for identifying outliers.\nBut it is also very sensitive to outliers.\nIf one value is drastically different for the others, the range can be misleading.\nFor example, see the histogram of GRE scores above.\nThis makes a major limitation of the range apparent: it is based on only two of the scores in the variable.\nExample\nGRE scores for two classes with the same mean (151.3) and same range (40):\n\n\nset.seed(1234)\nx1 <- rnorm(1000, 151.3, 8)\nx1 <- x1[x1 > 130 & x1 < 170]\nx2 <- c(rnorm(length(x1)-2, 151.3, 3), 130, 170)\npar(mfrow = c(1,2))\nhist(x1, col = \"skyblue\", xlim = c(130, 170), main = NULL)\nabline(v = mean(x1), col = \"red\", lwd = 2)\nhist(x2, col = \"skyblue\", xlim = c(130, 170), main = NULL, breaks = 15)\nabline(v = mean(x2), col = \"red\", lwd = 2)\n\n\n\npar(mfrow = c(1, 1))\n\n\n\n\n\nrange(x1)\n\n\n[1] 130 169\n\nrange(x2)\n\n\n[1] 130 170\n\nQuantiles\nQuantiles are a set of values in a variable that divide it into equal groups.\nThe most common is quartiles, which divide a variable into four equal parts, so that there are the same number of scores in each quartile.\nThe lower or first quartile separates the lower 25% of the scores from the upper 75%,\nthe second or median quartile – which is the median value – separates the lower and upper 50%,\nand the third or upper quartile separates the lower 75% from the upper 25%.\n\nThese three quartiles separate the variable into 4 equal parts.\nFor example, we can get the quartiles of using the quantiles() function in R. Here I do this for x1above.\n\n\nquantile(x1)\n\n\n  0%  25%  50%  75% 100% \n 130  146  151  156  169 \n\nquantile(x2)\n\n\n  0%  25%  50%  75% 100% \n 130  149  151  153  170 \n\nInterquartile Range\nThe interquartile range is simply the range of values from the \\(25^{th}\\) and \\(75^{th}\\) quartiles. A benefit of this measures is that is is not as sensitive to outliers as the range. For the above example, the interquartile range is 146 - 156. Notice that the \\(50^{th}\\) quantile, which is the second quartile, is the median:\n\n\nquantile(x1, probs = c(.25, .75))\n\n\n25% 75% \n146 156 \n\nmedian(x1)\n\n\n[1] 151\n\nVariance\nA major limitation of the range and interquartile range is that they are calculated with only two values in the data. What we really want is a measure of dispersion that takes into consideration all the values in the variable. The variance does this. The variance is a very important measure of variability. It is also closely related to the standard deviation, which we talk about next. In words the variance is the average squared deviation But let’s try to understand why we calculate the variance the way we do. To do that we need to understand deviations.\nDeviance\nWe can calculate the distance between the mean and each score. These distances are called deviations. Each score has a deviation. I have plotted the histogram of the deviations for each of the two classes GRE scores below.\n\n\npar(mfrow = c(1,2))\nd1 <- x1 - mean(x1)\nd2 <- x2 - mean(x2)\nhist(d1, col = \"skyblue\", xlim = c(-30, 30), main = NULL)\nabline(v = mean(d1), col = \"red\", lwd = 2)\nhist(d2, col = \"skyblue\", xlim = c(-30, 30), main = NULL, breaks = 15)\nabline(v = mean(d2), col = \"red\", lwd = 2)\n\n\n\npar(mfrow = c(1, 1))\n\n\n\nWe might think to just take the mean of the deviations as a measure of the average or “typical” distance from the mean for each set of scores. But, the mean deviation for the first score is 0 and the mean deviation for the second set is 0. This is because, being a measure of central tendency, the mean is in the middle, and the positive distances of scores above the mean cancel out the negative distances below the mean.\nWe could take the mean of the absolute values of the deviations, but a more ingenious solution is to square the deviations.\nThis does two things:\nSquaring takes care of the problem of the deviations summing to 0, as all the squared deviations will be positive. Remember, positive times a positive is a positive, but a negative times a negative is also a positive.\nBecause deviations are squared, numbers further from the mean have a greater influence than those closer to the mean. Therefore, deviations are sensitive to outliers. This can be good or bad, depending on the situation.\nThe sum of the squared deviations is often referred to simply as the “sum of squares”, and symbolized as \\(SS\\). The sum of squares is not very meaningful by itself, so we often calculate the mean squared deviation, by dividing the \\(SS\\) by \\(N\\). This give us:\nPopulation Variance\n\\[\n\\sigma^2 = \\frac{SS}{N} = \\frac{\\Sigma(x - \\mu)^2}{N}\n\\]\nSample Variance\n\\[\ns^2 = \\frac{\\Sigma{(x - \\bar{X})^2}}{n - 1}\n\\]\nComparing formulae for mean and variance\n\\[\n\\mu = \\frac{\\Sigma x}{N}, \\quad \\sigma^2 = \\frac{\\Sigma(x - \\mu)^2}{N}.\n\\]\nComparing the formulae for the mean and variance makes clear that the variance is the mean squared deviation.\nStandard Deviation\nIf the variance is the mean squared distance from the mean, then taking the square root of the variance gives us the mean, or average, distance from the mean.\nPopulation standard deviation\n\\[\n\\sigma = \\sqrt{\\sigma^2} = \\sqrt{\\frac{SS}{N}} = \\sqrt{\\frac{\\Sigma{(x -\\mu)^2}}{N}}. \n\\]\nSample standard deviation\n\\[\ns = \\sqrt{s^2} = \\sqrt{\\frac{SS}{n-1}} = \\sqrt{\\frac{\\Sigma{(x - M)^2}}{n-1}}. \n\\]\nStandard Deviation\n\n\npar(mfrow = c(1,2))\nhist(x1, col = \"skyblue\", xlim = c(130, 170), main = NULL)\nabline(v = mean(x1), col = \"red\", lwd = 2)\nhist(x2, col = \"skyblue\", xlim = c(130, 170), main = NULL, breaks = 15)\nabline(v = mean(x2), col = \"red\", lwd = 2)\n\n\n\npar(mfrow = c(1, 1))\n\n\n\nThe variance of x1 is 54.71 and the variance of x2 is 9.51. The standard deviation of x1 is 7.4 and the standard deviation of x2 is 3.08.\nStandard Deviation\n\n\nd <- data.frame(x1, x2)\ndescribe(d, fast = TRUE)\n\n\n   vars   n mean  sd min max range   se\nx1    1 980  151 7.4 130 169    39 0.24\nx2    2 980  151 3.1 130 170    40 0.10\n\n\n\n\n",
    "preview": "posts/2020-09-23-variability/distill-preview.png",
    "last_modified": "2021-01-26T12:28:16-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-09-08-exploring-data-graphically/",
    "title": "Exploring Data Graphically",
    "description": "Before we start simplifying our data with descriptive statistics and then models, we should always explore all the data. A great way to do that is graphically.",
    "author": [
      {
        "name": "William Murrah",
        "url": "www.statistical-thinking.com"
      }
    ],
    "date": "2020-09-08",
    "categories": [],
    "contents": "\n\n\nlibrary(mosaic)\n\n\n\nBefore you start simplifying your data with descriptive statistics, it is essential that you explore all your data. A reasoned use of graphics is indispensable for this purpose. In this tutorial, I will demonstrate some of the most commonly used graphical methods to explore experimental data.\nR has extensive methods for graphics, which allow for amazing visualizations of data. My purpose here is to teach you the basics of graphics, so I will stick with some relatively simple graphics in base R.\nBar Graphs\nTypically, the type of graph you use depends on the type of data you are exploring.\nWe will use the mtcars data, which is part of base R. Below, I load the data, then transform two variables to factors, which is how R handles categorical variables. Take a look at the help file for mtcars by typing ?mtcars into the R console. Look at the data description under the “Format” heading to see how I decided to label the two factors vs and am. You should also read through this list of variable names and labels to understand what our data are measuring.\n\n\ndata(mtcars)\n\nmtcars <- transform(mtcars,\n                    vs = factor(vs, labels = c(\"v-shaped\", \"straight\")),\n                    am = factor(am, labels = c(\"automatic\", \"manual\")))\n\n\n\nWhen we have a categorical variable, we often want to know how many cases are in each category. In R a categorical variable is called a factor, and the categories within that variable are called levels. Let’s look at the vs factor, which describes the shape of the engine for each car. First, I will simply table this variable:\n\n\ntable(mtcars$vs)\n\n\n\nv-shaped straight \n      18       14 \n\nWe can see that we have 18 cars with a V-shaped engine and 14 with a straight engine. Next, I will use the table function to create a bar plot.\n\n\nbarplot(table(mtcars$vs))\n\n\n\n\nNote, that the bar plot needs tabled data, so the table() function is nested within the first argument to this plot.\n\n\ntab <- table(mtcars$vs, mtcars$am)\ncolnames(tab)\nbarplot(tab, beside = TRUE, legend.text = rownames(tab), \n        main = \"Comparing engine shape to transmission type\", )\n\n\n\nHistograms\nSimilar to the bar graph is the histogram. The major difference is that histograms are most often used with continuous variables, instead of categorical variable. While bar graphs have natural groupings (the categories or levels), continuous variables do not. So, to create a histogram a continuous variable is binned into groups and the frequencies of cases in the range of those groups is plotted with a bar.\n\n\nhist(mtcars$mpg)\n\n\n\n\nIt is very important to remember that when you create a histogram a decision is being make about how to bin the data. This can impact the shape of the distribution, and your interpretation. It is often a good idea to play around with this binning. To do that in the R hist() function you can use the breaks = argument. This allows you to set the number of bins. For example, compare the two histograms of mpg.\n\n\nhist(mtcars$mpg, breaks = 3, main = \"Histogram pf mpg with 3 breaks\")\n\n\n\nhist(mtcars$mpg, breaks = 13, main = \"Histogram of mpg with 13 breaks\")\n\n\n\n\nAlso note that I called for 13 bins, but was given less than that. So, the hist() function only takes this value as a suggestion. See ?hist for more information.\n\n\nplot(density(mtcars$mpg))\n\n\n\nBoxplots\nBoxplots or box and whisker plots, are also useful for exploring the distribution of continuous variables. These plots visualize the median, interquartile range, the full range, and look for extreme values.\n\n\nboxplot(mtcars$mpg)\n\n\n\n\n\n\n(qtle <- quantile(mtcars$mpg))\n\n\n    0%    25%    50%    75%   100% \n10.400 15.425 19.200 22.800 33.900 \n\nboxplot(mtcars$mpg, horizontal = TRUE)\ntext(x = qtle[1], y = 1.3, labels = paste(\"0%\\n\", qtle[1]))\ntext(x = qtle[2], y = 1.3, labels = paste(\"25%\\n\", qtle[2]))\ntext(x = qtle[3], y = 1.3, labels = paste(\"50%\\n\", qtle[3]))\ntext(x = qtle[4], y = 1.3, labels = paste(\"75%\\n\", qtle[4]))\ntext(x = qtle[5], y = 1.3, labels = paste(\"100%\\n\", qtle[5]))\n\n\n\nhist(mtcars$mpg)\n\n\n\n\n\n\nboxplot(mpg ~ vs, data = mtcars)\n\n\n\n\n\n\nboxplot(mpg ~ vs * am, data = mtcars, las = 3, ann = FALSE, xaxt = \"n\")\ntitle(ylab = \"Miles per Gallon\", \n      xlab = \"Engine Shape and Transmission Type\",\n      main = \"Miles per Gallon by Engine Shape and Transmission Type\")\naxis(side = 1, at = 1:4, mgp = c(1,2,0), \n     labels =c(\"V-shaped\\nAutomatic\", \"straight\\nAutomatic\", \n                                   \"V-shaped\\nManual\", \"straight\\nManual\"))\n\n\n\n\nScatterplots\nScatterplots are useful when we want to compare two numeric variables. For example, let’s look at the relation between miles per gallon and horsepower.\n\n\nplot(mpg ~ hp, data = mtcars)\n\n\n\n\nNote that each point on the plot represents a single car, and indicates it’s horsepower on the x axis and it’s miles per gallon on the y axis. For example, the car represented by the left most point has a hp of about 50 and a mpg of a little more than 30. We see a negative relation between these two variables, a car with more horsepower is likely to have a lower miles per gallon.\n\n\n\n",
    "preview": "posts/2020-09-08-exploring-data-graphically/distill-preview.png",
    "last_modified": "2021-01-11T12:46:09-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-15-fiml-regression-auxiliary/",
    "title": "FIML in Lavaan: Regression Analysis with Auxiliary Variables",
    "description": "This is the third tutorial in a series that demonstrates how to us full information maximum likelihood (FIML) estimation using the R package `lavaan`.",
    "author": [
      {
        "name": "William Murrah",
        "url": "www.statistical-thinking.com"
      }
    ],
    "date": "2019-04-17",
    "categories": [],
    "contents": "\nIn this post, I demonstrate two methods of using auxiliary variable in a regression model with FIML. I am using data and examples from Craig Ender’s website Applied Missing Data. The purpose of these posts is to make the examples on Craig’s website, which uses Mplus, available to those who prefer to use lavaan\nMplus allows you to use auxiliary variable when using FIML to include variables that help estimate missing values with variables that are not part of the analytic model. There may be variables that are correlated with variables with missing values or variables that are predictive of missing. However, these auxiliary variable are not part of the model you wish to estimate. See Craig’s book Applied Missing Data Analysis for more information about auxiliary variables.\nI attended a workshop where Craig showed us how to use the auxiliary command in Mplus to make use of auxiliary variables. However, lavaan does not have this option. He also showed us what he called a ‘brute force’ method to include auxiliary variables in Mplus. Here is how to do it in lavaan.\nBrute Force Method\nThis model is the same as used in my last post, where job performance (jobperf) is regressed on wellbeing (wbeing) and job satisfaction (jobsat). In this example these three variables are the only ones which we want to model. However, tenure and IQ are related to missingness in these variables. So, we want to use them to help us better estimate our model of interest. If we included them as predictors in the regression model, it would allow us to use all the available information in these five variables, but it would change the model substantially. We can use auxiliary variables to better estimate the original model.\nImport Data\nFirst we import data, name the variables, and recode the -99’s to NA.\n\n\n# employeeAuxiliary.R ---------------------------------------------------\n\n# R packages used\nlibrary(lavaan)\n# Import text file into R as a data frame.\n\nemployee <- read.table(\"path/to/file/employee.dat\")\n\n# Assign names to variables.\n\nnames(employee) <- c(\"id\", \"age\", \"tenure\", \"female\", \"wbeing\", \"jobsat\", \n                 \"jobperf\", \"turnover\", \"iq\")\n\n# Replace all missing values (-99) with R missing value character 'NA'.\nemployee[employee==-99] <- NA\n\n\n\nCreate Regression Model Object (Brute Force)\nBasically, the brute force method entails correlating the auxiliary variables with other auxiliary variable, the predictors, and the residuals for the outcome variable.\n\n\n# The b1* and b2* are labels used in the Wald test below\nmodel <- '\njobperf ~ b1*wbeing + b2*jobsat\nwbeing ~~ jobsat\nwbeing ~~ turnover + iq\njobsat ~~ turnover + iq\njobperf ~~ turnover + iq\nturnover ~~ iq\n'\n\n\n\nFit and Summarize the Model\n\n\nfit <- sem(model, employee, missing='fiml', fixed.x=FALSE, \n           meanstructure=TRUE)\nsummary(fit, fit.measures=TRUE, rsquare=T, standardize=T)\n\n\n\nWald test\nJust as we did in the previous post.\n\n\nlavTestWald(fit, \n            'b1 == 0\n             b2 == 0')\n\n\n\nUsing auxiliary Command in semTools\nFirst, load the semTools package\n\n\nlibrary(semTools)\n\n\n\nCreate Regression Model Object\nNext, create a model object with just the model of interest\n\n\nmodel2 <- '\njobperf ~ wbeing + jobsat\n'\n\n\n\nThen, create a vector of the names of the auxiliary variables\n\n\naux.vars <- c('turnover', 'iq')\n\n\n\nFit the Model\nThen, fit the model to the new model object.\n\n\nfit2 <- sem(model2, employee, missing='fiml', meanstructure=TRUE, fixed.x=FALSE)\n\n\n\nUsing this model object, fit another model that incorporates the auxiliary variables using the sem.auxiliary function from the semTools package.\n\n\nauxfit <- sem.auxiliary(model=fit2, aux=aux.vars, data=employee)\n\n\n\nFinally, summarize the model object that includes the auxiliary variables.\n\n\nsummary(auxfit, fit.measures=TRUE, rsquare=TRUE, standardize=TRUE)\n\n\n\nThere you have it! Two way to use auxiliary variables in a regression model using lavaan.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-11T11:51:05-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2019-01-13-prepare-spss-data-for-mplus/",
    "title": "Prepare SPSS Data for Mplus",
    "description": "This tutorial demonstrates how to prepare data from SPSS for use with Mplus.",
    "author": [
      {
        "name": "William Murrah",
        "url": "www.statistical-thinking.com"
      }
    ],
    "date": "2019-01-13",
    "categories": [],
    "contents": "\nWhen I have to prepare data for Mplus, I use the MplusAutomation package in R. Its great! I import the SPSS data file into R with the foreign package. Then I use the prepareMplusData() function to create a .dat file for use in Mplus. This function also creates basic Mplus code that can pasted into Mplus or a text file used to prepare Mplus code files. MplusAutomation has many other great features and I highly recommend it for those who use Mplus and R.\nBut many of my colleagues don’t use R, and therefore this option is not feasible. Recently I gave an informal talk to some colleagues on how to get data from SPSS to Mplus, which I thought might be useful to others who needed to manually prepare SPSS data for Mplus. Here, I include the steps I recommended with links to an SPSS syntax example.\nBefore describing the steps, I think it is important to point out the major differences between an SPSS data file and an Mplus data file. SPSS data files include variable names, variable labels and other information. They also may contain different types of variables including numeric, string, and date variables. Mplus data files are simply a tab delimited numeric matrix. No variable names and only numbers as data points. Variable names are supplied in the Mplus code and are not in the data file.\nBasic Steps\nThe following 6 steps can be used to get SPSS data ready for Mplus:\nMake a copy of the SPSS data file\nRecode any non-numeric variables you want to include in the Mplus data file\nRename variables longer than 8 characters\nDeal with missingness values (if necessary)\nSave SPSS data file as a tab delimited file (.dat)\nCreate basic code for Mplus\nCheck descriptive statistics and missingness patterns\n1. Make a copy of the SPSS file\nThis step is pretty simple. It is a good idea to keep an original copy of your data file. That way you can always start over. I will be using an example data file that contains a subset of cases and variables from the STAR public access data set. This data can be found in the R package AER. If you have R you can use the following code to generate the data file:\n\n\n\nYou can also find of copy of the SPSS file on my github page for this tutorial here. .\n2. Recode any non-numeric variables\nMplus only handles numeric data. This does not mean you can’t have categorical variables in your analyses, but they have to be coded with numbers. For example, the variable ‘gender’ in the star data frame consists of two string values: “female” and “male”. We can recode these so females have a value of ‘0’ and males have a value of ‘1’. The following SPSS syntax does the trick:\n    RECODE \n    gender\n    ('female' = 0)\n    ('male' = 1)\n    (MISSING=SYSMIS)\n     into male.\n    EXECUTE.\nNotice that I also renamed the variable ‘male’ to indicate the category the variable identifies. If you run across a variable named ‘gender’ coded as ‘0’s and ’1’s, you don’t know which value signifies females and which males. By naming the binary variable ’male’, I am indicating that ’1’s are males, therefore the ’0’s must be females. This is just good data management.\n3. Rename variables longer than 8 characters\nMplus only recognizes the first 8 characters of variable names. If the first 8 characters of each of your variables are unique, you might be okay with Mplus truncating your variable names. But if you have variables in which the first 8 characters are not unique you can run into major problems. For example, if you had variables named ‘kindergartenMath’ and ‘kindergartenRead’, Mplus would treat them as having the same variable name. So don’t overlook this step!\nThe following SPSS code renames the long variables in the star data:\n    RENAME VARIABLES \n    (ChildIdentification = childId)\n    (readKindergarten = readk)\n    (mathKindergarten = mathk).\n4. Deal with missing values\nThere are two types of missing data possible in an SPSS data frame. System missing data is indicated by a period (‘.’). You can also designate any numeric value as a user missing value (e.g. -99). You don’t necessarily have to change these values to prepare your data for Mplus. But you must at least know which values indicate missingness for EACH variable. I also note that I have experienced some difficulty with the period (‘.’) value as a missing indicator when preparing SPSS data for Mplus. I usually recode all missing values to one numeric value (e.g. -99, or -999) that is not in the range of possible values for any of my data. Later you will have to tell Mplus what values indicate missing data for your variables. It is much easier if this value is one number, and it is the same for all variables. However, you may have good reason to have different missing values. Again, just know what your missingness indicators are.\n5. Create a tab delimited file\nNow that you have an SPSS file in order, you need to save it as a tab delimited file. To do this you just need to click on the ‘File’ option in the SPSS dataSet menu,and then click ‘Save As’. In the ‘Save as type:’ menu on the resulting tab, select ’tab delimited (*.dat)‘. MAKE SURE THE ’Write variable names to speadsheet’ BOX IS NOT CHECKED! Mplus data files should only contain numbers. You can also use the ’Variables…’ radio to select only the variables you need in Mplus. This is useful if you have non-numeric variables in the original data file that you don’t want to use and therefore don’t what to waste time transforming. Below is a snippet of SPSS syntax that resulted from the above procedure on the star data.\n    SAVE TRANSLATE OUTFILE='C:\\Dropbox\\3_Teaching\\SPSS2MplusDemo\\starMplus.dat'\n      /TYPE=TAB\n      /MAP\n      /REPLACE\n      /CELLS=VALUES.\n6. Create basic code for Mplus\nFinally, you will want to create a basic set of code for Mplus that will be the basis of all your analyses used in Mplus. I will mention three things you want to make sure get into your Mplus code accurately. First, you need to include the path to your tab delimited file in the DATA section of your Mplus code file. Second, you want to make sure you have the variable names correct. Remember that Mplus data files only contain the numeric data. Variable names are assigned to each column of the tab delimited file with the VARIABLE command in the Mplus code file. To keep you from having to type in all the variable names, you can copy the column of variable names from the SPSS file (in Variable View) and paste them into EXCEL. Then use the transpose function in the ‘paste special’ menu to convert the column of names into a row of names. Then paste this row in the Mplus code file. Third, you need to tell Mplus what the missing data values are. Because I used -99 for the only missing value for ALL of my variables, the following code should be in the VARIABLE section of your Mplus code file:\n    MISSING ARE ALL (-99);\nI will mention one final peculiarity of Mplus that can trip up this process. Mplus only allows 80 characters on each line of code. So you may have to break up long rows into shorter rows. For example the file path and variable names often have to be broken up this way. If you don’t Mplus will give you an error message. If the data file (starMplus.dat) is in the same folder as the Mplus code file (let’s say we call it starBasic.inp) then the code file might look like this:\nTITLE:  STAR analysis\n\nDATA:\n  FILE IS \"starMplus.dat\";\n\nVARIABLE:\n  NAMES ARE childId readk   read1   mathk   math1\n  male  white other;\n  USEVARIABLES ARE readk-other;\n  MISSING ARE ALL (-99);\n  categorical are male-other;\nANALYSIS:\n  TYPE IS Basic;\n\nOUTPUT:  SAMPSTAT PATTERNS;\nplot: type = plot1 plot2;\n7. Check descriptive statistics and missingness patterns\nTo make sure your data has been correctly converted to the .dat file, run some descriptive statistics in both SPSS and Mplus. Because of the default methods each program uses to deal with missing data, you will probably have to do a few things to get an equivalent comparison. I ask SPSS to use listwise deletion for descriptive statistics. Then I do the same in Mplus. Take a look a the SPSS syntax file and the Mplus files that have ‘LW’ in the title for details of how to do this. Basically, it entails including something like the following code in SPSS:\n    DESCRIPTIVES VARIABLES=readk read1 mathk math1 male white other\n    /STATISTICS=MEAN STDDEV MIN MAX\n    /MISSING=LISTWISE.\nNotice the last line is a missing command, and is not available in the drop down menu. So you will have to use the syntax editor (but you should be doing that anyway!).\nFor Mplus include the following command as part of the data section of the input file:\n       LISTWISE = ON;\nYou can also compare the missingness patterns generated by SPSS and Mplus (see the ‘LW’ files) which should be the same. However, note that the orientation of patterns differs in the two programs.\nAnd that’s it! You can download all the code files for this demonstration from my github page found here. The ‘stars.sps’ and the ‘SPSSdescriptivesLW.sps’ files contain all the syntax needed to complete this tutorial.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-11T11:51:05-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-12-15-fiml-regression/",
    "title": "FIML in Lavaan: Regression Analysis",
    "description": "This tutorial demonstrates how to use full information maximum likelihood (FIML) estimation to deal with missing data in a regression model using `lavaan`.",
    "author": [
      {
        "name": "William Murrah",
        "url": "https://statistical-thinking.com"
      }
    ],
    "date": "2018-12-15",
    "categories": [],
    "contents": "\nImport Data\nIn this post I use FIML to deal with missing data in a multiple regression framework. First, I import the data from a text file named ‘employee.dat’. You can download a zip file of the data from Applied Missing Data website. I also have a github page for these examples here. Remember to replace the file path in the read.table function with the path to the text file location on your computer.\n\n\nemployee <- read.table(\"data/employee.dat\")\n\n\n\nBecause the original text file does not include variable names, I name the variables in the new data frame:\n\n\nnames(employee) <-  c(\"id\", \"age\", \"tenure\", \"female\", \"wbeing\", \"jobsat\", \n                     \"jobperf\", \"turnover\", \"iq\")\n\n\n\nthen I recode all data points with the value of -99 in the original text file, which indicates a missing value, to NA, the missing data value recognized by R.\n\n\nemployee[employee == -99] <-  NA\n\n\n\nCreate Regression Model Object\nNow we are ready to create a character string containing the regression model using the lavaan model conventions. Note that b1 and b2 are labels that will be used later for the Wald test. These labels are equivalent to (b1) and (b2) after these variables in the Mplus code.\n\n\nmodel <- '\n# Regression model \njobperf ~ b1*wbeing + b2*jobsat\n\n# Variances\nwbeing ~~ wbeing\njobsat ~~ jobsat\n\n# Covariance/correlation\nwbeing ~~ jobsat\n'\n\n\n\nIn addition to the regression model, I also estimated the variances and covariances of the predictors. I did this to replicate the results of the original Mplus example. In Mplus you have to estimate the variances of all of the predictors if any of them have missing data that you would like to model. In lavaan the fixed.x=FALSE argument has the same effect (see below).\nFit the Model\nNext, I use the sem function to fit the model.\n\n\nfit <- sem(model, employee, missing='fiml', meanstructure=TRUE, \n           fixed.x=FALSE)\n\n\n\nListwise deletion is the default, so the missing=‘fiml’ argument tell lavaan to use the FIML instead. I also included the meanstructure=TRUE argument to include the means of the observed variables in the model, and the fixed.x=FALSE argument to estimate the means, variances, and covariances. Again, I do this to replicate the results of the original Mplus example.\nGenerate Output\nWe are now ready to look at the results.\n\n\nsummary(fit, fit.measures=TRUE, rsquare=TRUE, standardize=TRUE)\n\n\n\nCompared to what we learned in the last post, the only thing new to the summary function is the rsquare=TRUE argument, which, not surprisingly, results in the model R2 being included in the summary output.\nI only show the Parameter estimates section here:\nParameter estimates:\n\n  Information                                 Observed\n  Standard Errors                             Standard\n\n                   Estimate  Std.err  Z-value  P(&gt;|z|)   Std.lv  Std.all\nRegressions:\n  jobperf ~\n    wbeing   (b1)     0.476    0.055    8.665    0.000    0.476    0.447\n    jobsat   (b2)     0.027    0.060    0.444    0.657    0.027    0.025\n\nCovariances:\n  wbeing ~~\n    jobsat            0.467    0.098    4.780    0.000    0.467    0.336\n\nIntercepts:\n    jobperf           2.869    0.382    7.518    0.000    2.869    2.289\n    wbeing            6.286    0.063   99.692    0.000    6.286    5.338\n    jobsat            5.959    0.065   91.836    0.000    5.959    5.055\n\nVariances:\n    wbeing            1.387    0.108                      1.387    1.000\n    jobsat            1.390    0.109                      1.390    1.000\n    jobperf           1.243    0.087                      1.243    0.792\n\nR-Square:\n\n    jobperf           0.208\nWald Test\nIn lavaan the Wald test is called separately from the estimation function. This function will use the labels assigned in the model object above.\n\n\n# Wald test is called seperately.\nlavTestWald(fit,  constraints='b1 == 0\n                               b2 == 0')\n\n\n\nResults of Wald Test\n$stat\n[1] 95.88081\n\n$df\n[1] 2\n\n$p.value\n[1] 0\nThere you have it! Regression with FIML in R. But, what if you have variables that you are not interested in incorporating in your model, but may have information about the missingness in the variables that are in your model? I will talk about that in the next post.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-11T11:51:05-06:00",
    "input_file": {}
  },
  {
    "path": "posts/2018-11-14-fiml-in-lavaan-descriptive-statistics/",
    "title": "FIML in Lavaan: Descriptive Statistics",
    "description": "A new article created using the Distill format.",
    "author": [
      {
        "name": "William Murrah",
        "url": "www.statistical-thinking.com"
      }
    ],
    "date": "2018-11-14",
    "categories": [],
    "contents": "\nFIML for Missing Data in Lavaan\nFull information maximum likelihood (FIML) is a modern statistical technique for handling missing data. If you are not familiar with FIML, I would recommend the book entitled Applied Missing Data Analysis by Craig Enders. The book is both thorough and accessible, and a good place to start for those not familiar with the ins and outs of modern missing data techniques.\nThe purpose of the FIML in Lavaan series of posts and the related git repository is to take some of the examples related to FIML estimation within a regression framework from the Applied Missing Data website, and translate them into code for the R package lavaan. The code on the Applied Missing Data website in mostly for Mplus, which is quite expensive software. I hope this will give those who don’t have access to Mplus the ability to work through the examples using free and open source software.\nIn this first tutorial I start with the basics: how to get descriptive statistics using FIML. The data and Mplus code for this example can be found on the Book Examples page of the Applied Missing Data website. I also created a github repository with the data and R files with equivalent code in lavaan, which can be found here. Remember to replace the file path in the R code below with the file path to the folder in which you unzip the data files.\nYou will also want to read over the lavaan documentation and visit the very helpful lavaan website to take advantage of the tutorials there. With these resources at your disposal, you should be able to use replicate the examples in lavaan. Here, I walk through the major sections of the R code. This is the same code found in the github repository in the R file entitled FIMLdescriptivesCorrelations.R.\nHeader\nI always include a header with basic information in my code files.\n\n\n#***********************************************************************\n# section 4.14 Summary Statistics --------------------------------------\n# Author: William M. Murrah\n# Description: This code replicates the section 4.14 example on the \n#              the appliedmissingdata.com website, which generates \n#              descriptive statistics and correlations,\n# Version history ------------------------------------------------------\n# 2014.05.30: code created\n# 2014.06.01: rewrote heading\n#***********************************************************************\n# R packages used\nlibrary(lavaan)\n\n\n\nImport and prepare data\nFirst, import the data into R. MPlus uses .dat files which can only contain numbers. Variable names are not included in the .dat file, but instead are included in the Mplus .inp file. I use the read.table function to read the .dat file.\n\n\n    employee <- read.table(\"data/employee.dat\")\n\n\n\nNext, I assign names to the variables in the new data frame.\n\n\n    # Assign names to variables.\n    names(employee) <- c(\"id\", \"age\", \"tenure\", \"female\", \"wbeing\", \n                         \"jobsat\", \"jobperf\", \"turnover\", \"iq\")\n\n\n\nThe final step in preparing the data is to recode the data values -99, which are used as missing data values in the .dat file, to NA, which is the missing value indicator in R.\n\n\n    # Replace all missing values (-99) with R missing value character 'NA'.\n    employee[employee==-99] <- NA\n\n\n\nCreate Model Object\nNow that the data are ready, I create a character string with the model using the lavaan syntax. For descriptives and correlations I model the mean, variances, and covariance/correlations.\n\n\n    # Create descriptive model object\n    model <- '\n    # means\n    age      ~ 1\n    tenure   ~ 1\n    female   ~ 1\n    wbeing   ~ 1\n    jobsat   ~ 1\n    jobperf  ~ 1\n    turnover ~ 1\n    iq       ~ 1\n    \n    # variances\n    age      ~~ age\n    tenure   ~~ tenure\n    female   ~~ female\n    wbeing   ~~ wbeing\n    jobsat   ~~ jobsat\n    jobperf  ~~ jobperf\n    turnover ~~ turnover\n    iq       ~~ iq\n    \n    # covariances/correlations\n    age      ~~ tenure + female + wbeing + jobsat + jobperf + turnover + iq\n    tenure   ~~ female + wbeing + jobsat + jobperf + turnover + iq\n    female   ~~ wbeing + jobsat + jobperf + turnover + iq\n    wbeing   ~~ jobsat + jobperf + turnover + iq\n    jobsat   ~~ jobperf + turnover + iq\n    jobperf  ~~ turnover + iq\n    turnover ~~ iq\n    '\n\n\n\nFit the Model\nTo fit the model, I use the lavaan sem function. This function takes the first two argument model and data. The third argument is missing ='fiml', which tells lavaan to use FIML (the default is to use listwise deletion).\n\n\n    fit <- sem(model, employee, missing='fiml')\n\n\n\nAlternatively, you could leave the section of the model code under the # means section and use the meanstructure=TRUE argument in the fit function as follows, which give the same results:\n\n\n    fit <- sem(model, employee, missing='fiml', meanstructure=TRUE)\n\n\n\nGenerate Output\nTo print the results to the console, use the summary function.\n\n\n    summary(fit, fit.measures=TRUE, standardize=TRUE)\n\n\n\nThe fit.measures=TRUE calls fit statistics in the output. This should look familiar to those who have used Mplus.\nlavaan (0.5-16) converged normally after 141 iterations\n\n  Number of observations                           480\n\n  Number of missing patterns                         3\n\n  Estimator                                         ML\n  Minimum Function Test Statistic                0.000\n  Degrees of freedom                                 0\n  P-value (Chi-square)                           1.000\n\nModel test baseline model:\n\n  Minimum Function Test Statistic              527.884\n  Degrees of freedom                                28\n  P-value                                        0.000\n\nUser model versus baseline model:\n\n  Comparative Fit Index (CFI)                    1.000\n  Tucker-Lewis Index (TLI)                       1.000\n\nLoglikelihood and Information Criteria:\n\n  Loglikelihood user model (H0)              -6621.805\n  Loglikelihood unrestricted model (H1)      -6621.805\n\n  Number of free parameters                         44\n  Akaike (AIC)                               13331.609\n  Bayesian (BIC)                             13515.256\n  Sample-size adjusted Bayesian (BIC)        13375.604\n\nRoot Mean Square Error of Approximation:\n\n  RMSEA                                          0.000\n  90 Percent Confidence Interval          0.000  0.000\n  P-value RMSEA <= 0.05                          1.000\n\nStandardized Root Mean Square Residual:\n\n  SRMR                                           0.000\nThe standardize=TRUE argument includes columns with standardized output. the std.all column in lavaan output is the same as the STDYX section in Mplus.\nParameter estimates:\n\n  Information                                 Observed\n  Standard Errors                             Standard\n\n                   Estimate  Std.err  Z-value  P(>|z|)   Std.lv  Std.all\nCovariances:\n  age ~~\n    tenure            8.459    0.858    9.865    0.000    8.459    0.504\n    female           -0.028    0.122   -0.229    0.819   -0.028   -0.010\n    wbeing            1.148    0.334    3.433    0.001    1.148    0.182\n    jobsat            0.861    0.340    2.531    0.011    0.861    0.136\n    jobperf          -0.330    0.308   -1.072    0.284   -0.330   -0.049\n    turnover         -0.377    0.116   -3.255    0.001   -0.377   -0.150\n    iq                0.674    2.066    0.326    0.744    0.674    0.015\n  tenure ~~\n    female           -0.052    0.071   -0.736    0.462   -0.052   -0.034\n    wbeing            0.569    0.195    2.916    0.004    0.569    0.155\n    jobsat            0.565    0.200    2.822    0.005    0.565    0.154\n    jobperf           0.061    0.178    0.344    0.731    0.061    0.016\n    turnover          0.016    0.066    0.240    0.810    0.016    0.011\n    iq                0.026    1.199    0.022    0.983    0.026    0.001\n  female ~~\n    wbeing            0.067    0.031    2.156    0.031    0.067    0.115\n    jobsat            0.028    0.031    0.881    0.378    0.028    0.047\n    jobperf          -0.009    0.029   -0.323    0.747   -0.009   -0.015\n    turnover          0.001    0.011    0.114    0.909    0.001    0.005\n    iq                0.284    0.192    1.481    0.139    0.284    0.068\n  wbeing ~~\n    jobsat            0.446    0.095    4.714    0.000    0.446    0.322\n    jobperf           0.671    0.084    8.030    0.000    0.671    0.456\n    turnover         -0.141    0.030   -4.768    0.000   -0.141   -0.257\n    iq                2.876    0.530    5.430    0.000    2.876    0.291\n  jobsat ~~\n    jobperf           0.271    0.080    3.378    0.001    0.271    0.184\n    turnover         -0.129    0.030   -4.248    0.000   -0.129   -0.234\n    iq                4.074    0.566    7.195    0.000    4.074    0.411\n  jobperf ~~\n    turnover         -0.203    0.028   -7.168    0.000   -0.203   -0.346\n    iq                4.496    0.523    8.588    0.000    4.496    0.426\n  turnover ~~\n    iq               -0.706    0.182   -3.872    0.000   -0.706   -0.180\n\nIntercepts:\n    age              37.948    0.245  154.633    0.000   37.948    7.058\n    tenure           10.054    0.142   70.601    0.000   10.054    3.222\n    female            0.542    0.023   23.817    0.000    0.542    1.087\n    wbeing            6.288    0.062  100.701    0.000    6.288    5.349\n    jobsat            5.950    0.063   94.052    0.000    5.950    5.053\n    jobperf           6.021    0.057  105.262    0.000    6.021    4.805\n    turnover          0.321    0.021   15.058    0.000    0.321    0.687\n    iq              100.102    0.384  260.475    0.000  100.102   11.889\n\nVariances:\n    age              28.908    1.866                     28.908    1.000\n    tenure            9.735    0.628                      9.735    1.000\n    female            0.248    0.016                      0.248    1.000\n    wbeing            1.382    0.107                      1.382    1.000\n    jobsat            1.386    0.108                      1.386    1.000\n    jobperf           1.570    0.101                      1.570    1.000\n    turnover          0.218    0.014                      0.218    1.000\n    iq               70.892    4.576                     70.892    1.000\nRecall that correlations are standardized covariances, so correlations are found in the std.all column in the Covariances section. Also, intercepts are means, and can be interpreted as the FIML means for the variables.\nFinally, to get the missing data patterns and covariance coverage output that can be included in Mplus output use the following code:\n\n\n    # Get missing data patterns and covariance coverage similar\n    # to that found in Mplus output.\n    inspect(fit, 'patterns') \n    inspect(fit, 'coverage')\n\n\n\nwhich leads to the following output:\nMissing Data Patterns\n    age tenure female wbeing jobsat jobprf turnvr iq\n160   1      1      1      1      1      1      1  1\n160   1      1      1      1      0      1      1  1\n160   1      1      1      0      1      1      1  1\nCovariance Coverage\n\n         age   tenure female wbeing jobsat jobprf turnvr iq   \nage      1.000                                                \ntenure   1.000 1.000                                          \nfemale   1.000 1.000  1.000                                   \nwbeing   0.667 0.667  0.667  0.667                            \njobsat   0.667 0.667  0.667  0.333  0.667                     \njobperf  1.000 1.000  1.000  0.667  0.667  1.000              \nturnover 1.000 1.000  1.000  0.667  0.667  1.000  1.000       \niq       1.000 1.000  1.000  0.667  0.667  1.000  1.000  1.000\n\n\n\n",
    "preview": {},
    "last_modified": "2021-01-11T11:51:05-06:00",
    "input_file": {}
  }
]
